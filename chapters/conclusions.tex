\chapter{Conclusions}\label{chapter:conclusions}

In this thesis, we have presented DARLInG (Domain Auto-labeling through Reinforcement Learning for the Identification of Gestures), our approach to domain-shift mitigation using unsupervised domain label generation through reinforcement learning.
Our model incorporates a Variational Autoencoder as the main encoder-decoder-classifier model while testing both Deep Deterministic Policy Gradient and Proximal Policy Optimization as the Reinforcement Learning agent for domain label generation.
As our signal-to-image transformation, we have investigated the use of Gramian Angular Fields, Markov Transition Fields, and Recurrent Plots.
We have tested this approach on a single-user leave-out split of the Widar 3.0 \cite{zheng2019zero} dataset.

We have ran 15 different configurations of our approach, with 12 of these utilizing an RL agent as a means for domain label generation.
In these configurations, the best result was using DDPG as the agent, MTF as the signal-to-image transformation, and one-hot as the domain embedding encoding.
Our results are inconclusive, but indicate that there \textit{may} be some promise to our approach.
What results we do have, though, show that DARLInG does generally increase performance, or at least does not negatively impact performance, over a standard VAE's performance at cross-domain gesture recognition.
We then analyzed why our approach may have not worked and suggest directions for future research which may be able to turn DARLInG into a viable approach for domain shift migitagion through unsupervised domain label generation.
As such, we conclude that, DARLInG as an approach to domain-shift mitigation, may have potential, although future research will be necessary to truly investigate whether DARLInG can truly be the darling approach its authors had originally hoped for.
\chapter{Literature Review}\label{chapter:literature-review}

In this chapter we review important works in the literature which form the foundation of this thesis.
We first discuss the initial set of works which cover Wi-Fi activity detection as well as other related works which do not use CSI data specificially before discussing those works which do utilize CSI data and publicly available datasets for this purpose.
We then look at various signal-to-image transformation methods which may be applied to time-series signal data, enabling the use of techniques from the image processing domain.
Finally, we look into domain shift mitigation methods and specifically reinforcement learning for domain shift mitigation.

\section{Wi-Fi for Activity Detection}

Past works have investigated the use of Wi-Fi signals generally for the purposes of activity detection.

The first work regarding the use of Wi-Fi signals for the detection of humans subjects we could find is the work of Chetty, Smith, and Woodbridge in 2012 \cite{chetty2011through}.
Their work utilized passive Wi-Fi signals propagating through a building with receivers placed outside the building for presence detection.
This method achieved reasonable results and proved that Wi-Fi signals could be used to detect human presence in buildings, although it required the indoor and outdoor APs to be synchronized through wires and was unable to detect precise activities of the human subjects.

The first work we could find discussing the use of Wi-Fi for activity detection is the work from Fadel Adib and Dina Katabi, published in 2013 \cite{adib2013see}.
This work shows the potential of using signals which could be produced by Wi-Fi APs to detect human activity from through a wall.
The most important idea in this work is the elimination of the radio ``flash'' which comes with the signal hitting a wall and bouncing back towards the transceiver.
Their work focused more on the radar technology implications and not on the use of consumer Wi-Fi APs for gesture detection.
They did, though, show that using matched filters was enough to perform rudimentary gesture recognition, given coarse enough gestures.

In the same year, a different group published a paper showing how to use signals in the 2.4 GHz range, i.e., compatible with Wi-Fi transceivers, for simple gesture detection using Doppler shift identification \cite{pu2013whole}.
This paper proposes the use of a narrowband pulse with a very narrow bandwidth of only a few Hertz and detecting the Doppler shift from the returned signal.
Using this method, the researchers were able to identify 9 different gestures with a claimed 94\% accuracy.

The same group as \cite{adib2013see} also published a separate paper in 2014 detailing the use of a custom-built Wi-Fi based device which could detect course body motions by leveraging the geometric position of its antennas and measuring values through a Time of Flight (ToF) approach \cite{adib20143d}.

Finally, it is also important to note that IEEE has a task group 802.11bf assigned specifically to standardize Wi-Fi sensing technologies \cite{du2022overview}.
This group is focused on standardizing the hardware requirements, specifically enabling CSI accessibility and specific measurement procedures which future devices can implement.
Their target is to standardize these requirements for future devices both in the sub-7 GHz range and in the 60 GHz range.
They additionally provide suggestions for what methods can be then be used to interpret the data provided, including the use of Fast Fourier Transform (FFT) algorithms to calculate a Channel Impulse Response from CSI and a Doppler FFT, which may be directly used for gesture recognition.
The standards for 802.11bf is set to be ratified and published by September 2024.

\section{Wi-Fi CSI for Gesture Recognition}

A selection of works which utilize Wi-Fi CSI data specifically for the task of gesture recognition are discussed in this section.

To the best of our knowledge, the first work discussing the use of CSI for gesture recognition was published in 2015 by He et al. \cite{he2015wig}.
This work looks into the use of CSI and outlier detection to detect gestures, achieving 92\% gesture recognition accuracy on four gestures in a line-of-sight experiment and 88\% accuracy in a non-line-of-sight experiment.

The 2019 work titled Person-in-WiFi from Wang et al. proposes the use of an array of three transmitter and three receiver antennas to directly predict body segmentation and pose estimation of persons located in between the aforementioned antennas \cite{wang2019person}.
In this work, they used an RGB camera to provide ground truth annotations.
The ground truth body segmentation masks were generated using Mask-RCNN while the Body-25 model of OpenPose was used for pose detection.
This work, shows that body segmentation and pose estimation is possible with only CSI data, achieving an mAP of 0.38 for body segmentation and around 0.1 meter error for join estimation. 
Qualitatively, the results are quite impressive and it is clear that at the very least, the model performs well given that its input data is one-dimensional.

Using the same dataset, Geng, Huang, and De La Torre published DensePose in 2022 performs similarly, but instead produces UV coordinates of the subjects.
This work also provides some interesting preprocessing steps on the raw CSI data to improve prediction performance.

WiGAN, published in 2020, proposes the use of a Generative Adversarial Network (GAN) to augment training data using the generator as well as using the discriminator as a feature fusion and extraction module \cite{jiang2020wigan}.
The output of the discriminator is then used fed into a Support Vector Machine (SVM) which classifies then gesture seen in the input data.
The authors claim that by using the discriminator, which fuses together multiple layers of a CNN module through yet another convolution, their method is able to ``learn and recognize the importance of different depth features by itself''.
On publicly available datasets, WiGAN indeed achieves better results than the competing methods at the time, achieving 98\% accuracy on Widar 3.0 with known subjects and $\approx$8--10\% lower performance on new domains.

The same group, in the same year, published DeepMV which proposes the use of multiple APs and audio sources, in the form of ultrasound signals, with a domain discriminator and embedding generator \cite{xue2020deepmv}.
This work is based on the intuition that the fusion of sensor data from multiple APs and audio sources placed around the room, which intuitively should provide more data.
The embedding generator produces a latent representation of the action which can then be used by a fully connected module to classify the action being performed while the domain discriminator uses the latent representation to predict the domain of the action.
A minimax game is played between the embedding generator and domain discriminator to minimize the maximum accuracy of the domain discriminator while maximizing the minimum accuracy of the action classifier.
On their self-collected dataset, they were able to achieve 83.7\% mean accuracy, outperforming all other benchmarked methods.

The 2021 paper by Zhuravchak et al. proposes the use of an LSTM as a classifier \cite{zhuravchak2022human}. 
In this method approach, CSI data is provided as an input with a fixed length and the LSTM provides a single output representing the action detected within the input window.
Their method achieves 87.8\% accuracy on a self-collected dataset.

Ma et al., published in 2021, proposes the use of a CNN and neural network state machine encoder and a LSTM trained using RL to eliminate the need of domain specific information \cite{ma2021location}.
This work also contains a curated benchmark of many previous works in this field.
Their proposal is the use of a neural network state machine relies on the assumption that there is a temporal dependency within and across CSI segments.
For example, it is unlikely that a person who is currently standing will immediately be sitting within the next CSI segment without a sit-down transition segment in between.
Although unlikely, the probability is non-zero, due to incontinuities in the data and mislabeling in the ground-truth labels, and thus a neural network state machine is used.
The results show >97\% mean accuracy on in-domain test samples with a drop of 14--17\% on out-of-domain samples.

Additionally, a few bachelors thesis' fromt he past years at the IRIS research cluster at TU/e have focused on this problem as well.
The 2022 thesis by van den Biggelaar proposes the use of reinforcement learning with Deep Q-Networks as the gesture classifier \cite{biggelaar2022gesture}.
Their result shows $\sim$88\% mean accuracy on Widar 3.0, dropping by $\sim$4\% on out-of-domain test samples.

The 2022 thesis by Oerlemans compares how different preprocessing methods appear to affect gesture recognition performance \cite{oerlemans2022effect}.
Specifically, signal filtering through a finite impulse response filter and phase unwrapping, transformation to a Doppler frequency spectrum (DFS), and transformation using Gramian Angular Fields (GAF) was explored.
On the SignFi dataset, they show that each of the above steps do indeed significantly improve model performance with the GAF transformation coupled with signal filtering resulting in the best performance.

\section{Wi-Fi CSI datasets for Gesture Recognition}

Three Wi-Fi CSI datasets for gesture recognition were considered for this thesis and they are each explained in this section.

The Widar 3.0 dataset, forthwith referred to as the Widar dataset for brevity, presents a dataset with developed specifically for ``cross-domain learning solutions'' \cite{zheng2019zero}.
The solution provided in this dataset is two-fold: 1) the (relatively) high number of domains that the data was collected with, and 2) the proposal of Body-coordinate Velocity Profile (BVP) with is a theoretically domain-independent representation of the data.
More details of BVP is discussed in Section \todo{Desecribe in background section}.
Finally, this paper also provides a baseline model to compare against.

SignFi is a dataset of Wi-Fi CSI data specifically for sign language recognition \cite{ma2018signfi}.
This dataset contains over 276 sign gestures in a lab and home environment with five different users.
A baseline CNN model is also presented in this paper, capable of achieving a $\sim$87\% mean accuracy over 150 sign gestures.

Person-in-WiFi is the dataset used in the paper by Wang et al. \cite{wang2019person}.
This dataset was made public and includes both Wi-Fi CSI data and RGB camera data of the activity from a fixed position.
This dataset was collected specifically for pose estimation solutions and is not meant specifically for activity recognition, as no activity labels are provided in the dataset.

\section{Signal-to-Image Transformations}

Different preprocessing methods have been investigated to transform raw tabular data into images for deep learning.
Four state-of-the-art approaches are DeepInsight \cite{sharma2019deepinsight}, REFINED \cite{bazgir2020representation}, GAF, and MTF \cite{wang2015imaging}.
A search of the current body of literature did not yield any research into a direct comparison of these techniques on a common dataset.
Instead, a previous unpublished work by the author of this thesis for the Seminar course at the TU/e has shown that these four methods performed best among state-of-the-art signal-to-image transformations \cite{satyawan2023cnns}.
A more thorough description of each of these methods are described in Section \todo{Describe in background section}

\section{Domain Shift Mitigation Methods}

There are a number of papers focusing on domain shift mitigation methods.
A selection of these papers, specifically those which are highly related to our problem domain, are discussed in this section.

The 2021 paper by Zinys et al. focuses on the use of GANs for domain shift mitigation, called Adversarial Domain Adaptation (ADA) \cite{zinys2021domain}.
In this work, the discriminator attempts to predict gesture and domain while the generator produces sample data.
The discriminator is provided a loss function based on ground truth data and accurately identifying generated data while the generator produces sample data which is in the same domain and gesture as its provided input.
Their results, tested on Widar show significantly better performance than the baseline Widar model on unseen domains.

Zhang et al., in 2022, proposes the use of federated learning for domain shift mitigation \cite{zhang2022wifi}.
The concept is to allow for each user to train their own neural networks and using matched average federated learning to combine all user models together.
Tested on the Widar dataset, they show performance competitive with state-of-the-art techniques.

Van Berlo et al. discusses attempts at using mini-batch alignment to generate domain factor independent latent representations of the data \cite{van2022insights}.
They showed that unfortunately, the proposed mini-batch alignment pipeline did not lead to better performance across domains.
The authors believe this may be due to a lack of sufficient domain factor information, leading to poor mini-batch alignment.
Alternatively, the assumptions of the underlying probability distributions may be incorrect.
In any case, it seems that this method, given current publicly available datasets, does not improve the SOTA.

Finally, the 2022 bachelors thesis by Sips investigates the use of network pruning and quantization for domain shift mitigation \cite{sips2022impact}.
The basic concept is to improve model robustness by enforcing sparsity and utilizing mixed precision training.
Tested against a baseline where sparsity and mixed precision training was not used, the modified networks performed slightly worse on in-domain test samples while they had mixed results on out-of-domain samples.

\section{Domain Shift Mitigation}

The following section contains some selected works in domain shift mitigation, mainly those related directly with RL of self-supervised techniques.

Zhang et al. in 2021 published research into using RL based features election for domain shift mitigation \cite{zhang2021adversarial}.
The proposed method would be able to select the most relevant features across two domains by employing Q-learning to learn policies for feature selection, utilizing the performance of a domain discriminator as its reward function.
By doing so, they attempt to align the feature manifolds between both domains.
Benchmarked on publicly available datasets, this method achieves the best mean accuracy among SOTA methods.

Martini et al. published a technique called ``Domain-Adversarial Neural Networks'' in 2021 \cite{martini2021domain}.
This technique uses a feature extractor, a domain classifier, and a label predictor.
The feature extractor maps the input to a latent representation, the domain classifier predicts the domain given the latent representation, and the label predictor provides a class prediction given the extracted features.
The loss function of the model balances the label predictor loss, using Cross Entropy Loss, and the domain classifier loss, also using Cross Entropy Loss, with the goal of providing a latent representation from the feature extractor which is domain-invariant, yet still discriminative such that the domain classifier is still capable of accurately classifying the domain.
Additionally, the paper proposes the use of Maximum Mean Discrepancy (MMD) to measure the difference between two domains.
The MMD measures the kernel-based difference between feature means of each domain.
This work proposes the use of MMD with the goal of minimizing this distance while maintaining the distinctness of each domain, allowing for the domain classifier to still work.
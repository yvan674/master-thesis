\chapter{Methodology}\label{chapter:methodology}

\begin{figure}
	\centering
	\includegraphics[width=\linewidth]{figures/arch_diagram.pdf}
	\caption{An abstracted diagram of the proposed DARLInG architecture.}
	\label{fig:arch-diagram}
\end{figure}

This chapter will first discuss the details of the dataset which we are using. 
We will discuss our approach to the problem of domain agnostic Wi-Fi CSI gesture classification and delve into the details of our chosen architecture.
This will first begin with a general overview of our method, then discuss each component of the architecture individually as well as their motivations and intuitions.

\section{Widar 3.0}
DO THIS

\section{DARLInG}

In this thesis, we present our novel approach DARLInG (Domain Autolabeling through Reinforcement Learning for the Identification of Gestures).
DARLInG is our proposed approach to domain-agnostic gesture identification using RL.
Our intuition comes from \cite{zhang2021adversarial} in which RL was used to identify features in the data which was invariant to domain shifts.
Analogously, we hypothesize that RL can then be used to identify those features which are \textit{not} invariant to domain shifts and produce a \textit{domain embedding} from said features.
We hypothesize that by providing our gesture discriminator with not only the original signal, encoded by a VAE-style encoder, but also this domain embedding, the gesture discriminator would be able to significantly increase its performance in gesture recognition throughout multiple domains.

The general architecture of DARLInG can be seen in Figure \ref{fig:arch-diagram} and the code can be found publicly on GitHub\footnote{\href{https://github.com/yvan674/DARLInG}{https://github.com/yvan674/DARLInG}}.
We first propose a signal preprocessing and signal-to-image transformation pipeline, described in Section \ref{sec:methodology-signal-preprocessing} and \ref{sec:methodology-signal-to-image}.
This transforms the input signal into an image that is then passed through a CNN encoder, as described in Section \ref{sec:methodology-signal-encoding}, encoding the signal into a latent space.
This latent representation is then used in two different ways: As an input for the multi-task learning heads, described in Section \ref{sec:methodology-multi-task-learning}, and as the state observation for our RL agent, described in Section \ref{sec:methodology-rl}.

\begin{figure}
	\centering
	\includegraphics[width=0.78\textwidth]{figures/rl_paradigm}
	\caption{The basic paradigm of Reinforcement Learning where an agent interacts with its environment through actions $a$ and receives a new state $s$ and reward $r$ in return.}\label{fig:rl-paradigm}
\end{figure}
Recall first that RL is typically modeled as a Markov decision process with an environment and observed state of that environment at time $t$, $s_t$. An agent then performs an action $a_t$ and is provided with a reward $r_t$ and a new observed state $s_{t+1}$.
A typical RL scenario is framed in this way and is visually represented by Figure \ref{fig:rl-paradigm}.

To mitigate domain-shift, our RL agent is tasked with producing the best possible domain embedding of the domain $\boldsymbol{d}_r = a, \boldsymbol{d}_r \in [0, 1]^e$ as its action where $e$ is the dimensionality of the domain embedding.
The domain embedding, or action, is generated by the RL agent given the signal latent representation $\boldsymbol{z} = s, \boldsymbol{z} \in [0, 1]^f$ as its state observation, where $f$ is the dimensionality of the latent representation.

To provide the RL agent with a reward function, inspired by triplet loss, we use two different multi-task learning modules.
The details of the reward function are described in Subsection \ref{subsec:methodology-reward}.
One of these modules is provided $\boldsymbol{d}_{\emptyset}$, representing a vector of zeros when the $d$ is one-hot encoded or a value of $\frac{1}{f}$ when $\boldsymbol{d}_r$ is a probability measure, and the other $\boldsymbol{d}_{r}$.

%\todonotestodo[inline]{This chapter is not finalized, due to the possibility of changing things around so will be kept as bullet points at this stage.}

\section{Signal Preprocessing}\label{sec:methodology-signal-preprocessing}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/signal_preprocessing_diagram.pdf}
	\caption{Details of the signal preprocessing module. The module is comprised of traditional signal preprocessing and a signal-to-image transformation.}
	\label{fig:signal-preprocessing-diagram}
\end{figure}

As the old adage goes, garbage in, garbage out.
As such, we propose a signal preprocessing module, visualized in Figure \ref{fig:signal-preprocessing-diagram}.
This module first cleans the CSI signal using traditional signal processing and then transforms it into an image for use by the signal encoder.

\subsection{Signal Processing}

\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/amp_original}
		\caption{The original amplitude signal.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/amp_step_1}
		\caption{The original amplitude signal and the low pass filtered signal.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/amp_step_2}
		\caption{The low pass filtered signal and the signal after the standard scalar.}
	\end{subfigure}
	\hfill
	\caption{An example of the amplitude shift signal after being transformed by each step of the signal processing pipeline. Only the first 600 samples of one channel from one tranceiver link is shown here for illustrative purposes.} \label{fig:amp-pipeline}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/phase_original}
		\caption{The original phase signal.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/phase_step_1}
		\caption{The original phase signal and the phase unwrapped signal.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/phase_step_2}
		\caption{The phase unwrapped signal and the phase filtered signal.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/phase_step_3}
		\caption{The phase filtered signal and the low pass filtered signal.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth]{figures/phase_step_4}
		\caption{The low pass filtered signal and the signal after the standard scalar.}
	\end{subfigure}
	\hfill
	\caption{An example of the phase shift signal after being transformed by each step of the signal processing pipeline. Only the first 600 samples of one channel from one tranceiver link is shown here for illustrative purposes.} \label{fig:phase-pipeline}
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=0.8\textwidth]{figures/phase_unwrap}
	\caption{A plot over time of the phase shift after unwrapping of all training data, colored by gesture, showing how there is no general trend of only increasing or decreasing phases.}\label{fig:phase-unwrap}
\end{figure}

The CSI data is provided as a set of complex values sampled at 1000 Hz.
We first decompose this into its radius and angle components on the complex plane or its amplitude and phase shifts, respectively.
We process this signal through two different pipelines, one for the amplitude component and one for the phase component, as the phase requires a few additional processing steps compared to the amplitude.

The pipeline for amplitude consists of the following steps:

\begin{enumerate}
	\item \textbf{Low Pass Filter} set to a cutoff frequency of 25 Hz and an order of 4.
	\item \textbf{Standard Scalar} which has been trained on all training data and transforms the signal to have a mean of 0 and a standard deviation of 1.
\end{enumerate}

The low pass filter is used to eliminate noise inherent in CSI data from environmental factors.
It has also been set to these values based on empirical experimentation which shows that no human movements during the performance of the 6 gestures which we test for has a frequency above 250 Hz.

The standard scalar is used since empirical evidence shows that neural networks work best when input values are close to the interval $[-1, 1]$ \cite{varun2023tuning}.

The effect of each step of the amplitude pipeline can be seen in Figure \ref{fig:amp-pipeline}.

The pipeline for the phase consists of the following steps prepended to the amplitude pipeline:
\begin{enumerate}
	\item \textbf{Phase Unwrapping} which unwraps the phase and makes the signal continuous.
	\item \textbf{Phase Filtering} step, which applies a uniform and median filter onto the phase shift signal, inspired by \cite{oerlemans2022effect}.
\end{enumerate}

The effect of each step of the phase pipeline can be seen in Figure \ref{fig:phase-pipeline}.

The signal can now be considered clean, or at least clean enough that we can continue with further steps.

During the course of experimentation, we considered taking the derivative of the phase, to eliminate a generally monotonously increasing or decreasing signal, as can been seen in the example in \ref{fig:phase-pipeline} after phase unwrapping.
Further investigation showed that this is actually not necessary as most signals do not monotonously increase or decrease.
A plot of the phase shifts after unwrapping of all signals can be seen in Figure \ref{fig:phase-unwrap}.
The plot clearly shows that there is no general trend of phases increasing or decreasing infinitely, with many phase shifts staying around 0 after unwrapping.

\subsection{Signal-to-Image Transformation}\label{sec:methodology-signal-to-image}

%Four crazy signal-to-image transformation methods! You won't believe number three!\todo{If there are no objections, this will be in the final paper}

The next stage in our method is transform the signal into an image, leveraging advances from computer vision.
There are many works from the literature which show that there are certainly improvements that can be gained from performing image processing on temporal data.
We will experiment with the following four methods for signal-to-image transformation: Gramian Angular Fields (GAF) \cite{wang2015imaging}, Markov Transition Fields (MTF) \cite{wang2015imaging}, and Recurrent Plots (RP) \cite{eckmann1995recurrence}.
A more comprehensive description of each of these transformations can be found in Section \ref{sec:background-signal-to-image}.

We process these images in Python using the pyts package \cite{faouzi2020pyts}, which conveniently contains ready-to-use implementations of all three of the aforementioned transformations.
Regardless of the chosen transformation, the signal is transformed into a two-dimensional tensor which can be treated as an image.
We also downsample this image to a size of $40 \times 40$ pixels for computational complexity purposes, which still provides a temporal resolution double that of the provided BVP data in Widar 3.0.
We can proceed then proceed with the encoding of this image into a latent space.

\section{Signal encoding}\label{sec:methodology-signal-encoding}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth]{figures/vae-diagram}
	\caption{The CNN-based VAE used in DARLInG. The number of ConvBlock elements is variable, depending on the experiment being performed. The reparameterization trick is the same trick described in \cite{kingma2013auto}.}\label{fig:vae-diagram}
\end{figure}

The encoding of the signal, now an image, into a latent space is performed using a Convolutional Neural Network (CNN)-based VAE.
The CNN itself is structured as a standard, deep CNN, made up of multiple \verb|ConvBlock|s where each block is made up of a \verb|Conv2d| with dropout, a batch norm layer, and an activation layer.
The number of input channels, filters, and kernel size of the \verb|Conv2d| layer and its dropout is set dynamically and differs by experiment.
The activation layer also differs by experiments and is either ReLU, LeakyReLU, or SeLU.
The number of \verb|ConvBlock|s is set dynamically and differs by experiment.

These blocks are then followed by two sets of two fully connected layers with a hidden layer size of 8192.
One set of these fully connected layers serve to predict the $\mu$ of the other set to predict the $\log \sigma$ of the latent variables.
The reparameterization trick is then used, adding a random variable $\mathcal{N}(0, 1)$ to produce the latent variables $\boldsymbol{z}$.
A visualization of the CNN-based VAE used can be seen in Figure \ref{fig:vae-diagram}.

The latent representation $\boldsymbol{z}$ of this signal is then passed to both the reinforcement learning agent as its state observation as well as to the multi-task learning modules.
How the RL agent uses this state observation is described in Section \ref{sec:methodology-rl} while its use by the multi-task learning module is described in Section \ref{sec:methodology-multi-task-learning}.

\section{Multi-task Learning}\label{sec:methodology-multi-task-learning}

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{figures/multitask_learning_module_diagram.pdf}
	\caption{Details of the multi-task learning module. The generator is made up of a series of deconvolutional layers and produces the BVP prediction while the classifier is a fully connected network and produces a probability distribution over each gesture class.}
	\label{fig:multitask-learning-module-diagram}
\end{figure}

The actual gesture classification module is built using a multi-task learning module, a visualization of which can be seen in Figure \ref{fig:multitask-learning-module-diagram}.
The idea, intuitively, is to enforce some sort of representation that is already approaching a domain-invariant representation of the data.
To do so, we use the generation of the BVP as an auxiliary task while keeping gesture recognition as our main task.
This is done as BVP is theoretically domain-independent \cite{zheng2019zero}, even though we are ultimately not interested in the BVP.
This approach is inspired by Martini et al. \cite{martini2021domain}, which, although using a different metric to modulate domain-independence as an optimization objective, suggests that an adversarial approach modulating both a domain-independence objective and a domain-discrimination objective can be powerful and increase classification performance.
The BVP generation is done by a series of deconvolutional layers while the classifier is a series of fully-connected layers.
Additionally, it has been shown empirically that multi-task learning can produce better results in each target task as opposed to dedicated networks \cite{tuggener2021deepscoresv2}.
This is likely due to the encoder being guided towards producing a more robust or efficient latent representation capable of being used for many different tasks instead of a latent representation focused on only one task.

Therefore, as BVP generation is a theoretically domain-independent process, we should expect the latent representation to contain both features which are completely domain independent as well as features which are not.
We do not, however, want to enforce domain-independence directly on the latent representation, as this may result in worse performance \cite{van2022insights}.
Further, \cite{martini2021domain} supports giving the latent representation domain-discriminatory powers.

As seen in Figure \ref{fig:arch-diagram}, we utilize two multi-task learning modules. The module receiving $\boldsymbol{d}_\emptyset$ is termed the \textit{null head} while the module receiving $\boldsymbol{d}_r$ is termed the \textit{embed head}. 
The motivation behind the use of two heads is such that we can measure the RL agent's performance in an unsupervised manner by comparing the performance of the null head to the embed head.
Intuitively, we should expect the embed head to perform better if the embedding $\boldsymbol{d}_r$ provided by the RL agent is able to provide some useful information, likely with respect to the domain, as opposed to the null head, which receives no useful information.
This comparison of the performance of the two embed heads is inspired by triplet loss.

For the loss function, we use cross entropy loss on the gesture classifier and mean squared error loss on the BVP generation.

\section{Reinforcement Learning Agent}\label{sec:methodology-rl}

To mitigate domain shift, we implement a novel method for unsupervised domain representations through reinforcement learning.
Recall the common terminology used in RL: the agent, the action $a$, the state observation $s$, and the reward $r$.

\begin{itemize}
	\item Base terminology: Agent, action, state observation (state), and reward
	\item The encoder-decoder architecture described in Section \ref{sec:methodology-signal-encoding} and \ref{sec:methodology-multi-task-learning} is the environment.
	\item RL loop:
	\begin{enumerate}
		\item The agent receives the image embedding produced by the encoder/backbone as its state
		\item The reinforcement learning agent produces an embedding of the domain as its action
		\item The environment is trained with one pair of heads receiving the action of the agent while the other pair receives senseless values (either all 0s if the representation is one-hot or a uniform distribution if the representation is a probability measure)
		\item After training of the agent, the RL agent is provided a reward from the environment which is used to improve the agent
	\end{enumerate}
	\item Let a performance metric function $M$ denote the gesture classification performance of a multi-task learning module given the gesture classification prediction $\hat{y_{c}}$. Then, the reward function $R$ of the agent is $R\left(M\left(\hat{y}_{c,rl}, y_c\right),M\left(\hat{y}_{c,0}, y_c\right)\right)$ and calculates the gesture prediction performance difference between the module given the domain embedding and the module given the null embedding.
	\begin{itemize}
		\item The intuition is, the reward is based on how much better the head pair with the action should perform than the head pair without the action
		\item This is based on the idea behind triplet loss, where in an unsupervised scenario, model performance can be determined by the metric performance differential between two inputs. Maximizing this difference implies better total model performance.
		\item We will also experiment with L1 vs L2 regularization on the reward, representated by the function $R$.
	\end{itemize}
	\item To speed up training, we take a page from AutoML competitions and the environment is trained within 1 minute and be focused on improving performance as fast as possible instead of achieving the best performance, at least during hyperparam optimization
	\item After hyperparameters are chosen, then we train properly and fully.
	\item A reward is then calculated using the function $R\left(M\left(\hat{y}_{rl}, y\right)-M\left(\hat{y}_{0}, y\right)\right)$, where $M$ is the metric used to calculate the performance of the gesture classifier in each multi-task learning module and $R$ is some reward function.
	\item We do not use the absolute difference, as we are interested in the having negative values representing the module provided $D_0$ having better performance.
	\item We will be looking at PPO \cite{schulman2017proximal} and SARSA \cite{rummery1994line}
	\begin{itemize}
		\item We've used both these techniques before in a previous project, so implementation should be relatively straightforwards
	\end{itemize}
	\item \emph{Motivation}
	\begin{itemize}
		\item We use the RL agent inspired by \cite{ma2021location} and hypothesize that the RL architecture there can be extended to domain auto-labeling
		\item We use the triplet-loss approach since triplet loss is a proven way to provide a numerical value showing the difference between two input
		\begin{itemize}
			\item In other words, this is analogous to testing the null hypothesis, where $H_0$ is no RL agent produced domain embedding and $H_1$ is a RL agent produced domain embedding
			\item Our hypothesis is that the domain embedding will produce better results
			\item If the RL agent can be trained, then the difference $(M\left(\hat{y}_{rl}, y\right)-M\left(\hat{y}_{0}, y\right)$ will approach $M\left(\hat{y}_{rl}, y\right)$ producing the maximum reward for the RL agent
		\end{itemize}
	\end{itemize}
\end{itemize}

\subsection{Reward Function}\label{subsec:methodology-reward}

TODO

\section{Model Training}\label{sec:methodology-training}
todo

